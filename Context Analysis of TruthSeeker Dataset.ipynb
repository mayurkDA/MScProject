{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select relevant columns for Bot Behavior Analysis\n",
    "feature_cols_bot = ['followers_count', 'friends_count', 'favourites_count', 'statuses_count', 'listed_count', 'following']\n",
    "X_bot = joined_df[feature_cols_bot]\n",
    "y_bot = joined_df['BotScore']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_bot, X_test_bot, y_train_bot, y_test_bot = train_test_split(X_bot, y_bot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf_model_bot = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model_bot.fit(X_train_bot, y_train_bot)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_bot = rf_model_bot.predict(X_test_bot)\n",
    "\n",
    "# Evaluate the model (MSE and RMSE)\n",
    "mse = mean_squared_error(y_test_bot, y_pred_bot)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "cv_scores = cross_val_score(rf_model_bot, X_train_bot, y_train_bot, cv=10, scoring='neg_mean_squared_error')\n",
    "print(f'Mean CV MSE: {-np.mean(cv_scores)}')\n",
    "print(f'Standard Deviation of CV MSE: {np.std(cv_scores)}')\n",
    "\n",
    "# Grid Search for Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [10, 20, 30, None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_model_bot, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_bot, y_train_bot)\n",
    "\n",
    "# Generate and Plot Learning Curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    grid_search.best_estimator_, X_train_bot, y_train_bot, cv=5, scoring='neg_mean_squared_error',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = np.mean(-train_scores, axis=1)\n",
    "train_std = np.std(-train_scores, axis=1)\n",
    "val_mean = np.mean(-val_scores, axis=1)\n",
    "val_std = np.std(-val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training error')\n",
    "plt.plot(train_sizes, val_mean, label='Validation error')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='gray')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color='gainsboro')\n",
    "\n",
    "plt.title('Learning Curve for Random Forest Regressor')\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Store the BotScore predictions for the entire dataset in a new column\n",
    "predicted_BotScore = grid_search.best_estimator_.predict(X_bot)\n",
    "joined_df['predicted_BotScore'] = predicted_BotScore\n",
    "\n",
    "# Calculate accuracy for bot detection\n",
    "y_pred_binary = np.where(y_pred_bot >= 0.5, 1, 0)\n",
    "y_test_binary = np.where(y_test_bot >= 0.5, 1, 0)\n",
    "\n",
    "accuracy = np.mean(y_pred_binary == y_test_binary)\n",
    "print(f'Accuracy for Bot Detection: {accuracy * 100:.2f}%')\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn import Softmax\n",
    "import torch\n",
    "\n",
    "# Initialize the BERT tokenizer and model for sequence classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# Softmax function for probabilities\n",
    "softmax = Softmax(dim=1)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = softmax(logits)\n",
    "    sentiment_score = torch.argmax(probabilities)  # You can map this to your preferred range/scale\n",
    "    return sentiment_score.item()\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "joined_df['sentiment_score'] = joined_df['tweet'].apply(get_sentiment)\n",
    "\n",
    "# Show some results\n",
    "print(joined_df[['tweet', 'sentiment_score']].head())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Preparing features and target variable\n",
    "feature_cols = ['mentions', 'quotes', 'replies', 'retweets', 'favourites', 'hashtags', 'sentiment_score']\n",
    "X = joined_df[feature_cols]\n",
    "y = joined_df['majority_target']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'Accuracy for predicting majority_target using XGBoost: {accuracy_xgb}')\n",
    "\n",
    "# Store the predictions back into the DataFrame\n",
    "joined_df['predicted_majority_target'] = xgb_model.predict(X)\n",
    "\n",
    "# Preparing features and target variable\n",
    "feature_cols = ['predicted_BotScore', 'sentiment_score', 'predicted_majority_target']\n",
    "X = joined_df[feature_cols]\n",
    "y = joined_df['BinaryNumTarget']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy for predicting Fake News in Context: {accuracy}')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Already calculated accuracy\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Calculate and print other evaluation metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
