{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51567085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "features_path = '/content/drive/MyDrive/TruthSeeker/Features_For_Traditional_ML_Techniques.csv'\n",
    "truth_seeker_path = '/content/drive/MyDrive/TruthSeeker/Truth_Seeker_Model_Dataset.csv'\n",
    "\n",
    "# Load the datasets\n",
    "features_df = pd.read_csv(features_path)\n",
    "truth_seeker_df = pd.read_csv(truth_seeker_path)\n",
    "\n",
    "# Display first few rows\n",
    "features_df.head(), truth_seeker_df.head()\n",
    "\n",
    "# Rename the unnamed columns to 'ID' in both DataFrames for easier reference\n",
    "features_df.rename(columns={features_df.columns[0]: 'ID'}, inplace=True)\n",
    "truth_seeker_df.rename(columns={truth_seeker_df.columns[0]: 'ID'}, inplace=True)\n",
    "\n",
    "# Drop duplicate columns ('tweet' and 'statement') from one of the DataFrames\n",
    "features_df_dropped = features_df.drop(columns=['tweet', 'statement'])\n",
    "\n",
    "# Perform the join operation using both 'ID' and 'BinaryNumTarget'\n",
    "joined_df = pd.merge(features_df_dropped, truth_seeker_df, on=['ID', 'BinaryNumTarget'])\n",
    "\n",
    "# Display the first few rows of the joined DataFrame\n",
    "joined_df.head()\n",
    "\n",
    "# Descriptive statistics\n",
    "joined_df.describe()\n",
    "\n",
    "# Information about the dataframe\n",
    "joined_df.info()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each unique value in the 'BinaryNumTarget' column\n",
    "value_counts = joined_df['BinaryNumTarget'].value_counts()\n",
    "\n",
    "# Create lists for the bar graph\n",
    "labels = ['True', 'Fake']\n",
    "counts = [value_counts.get(1, 0), value_counts.get(0, 0)]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, counts, color=['blue', 'red'])\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of True and Fake News Articles')\n",
    "plt.xlabel('News Type')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add text annotations on each bar\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count, str(count), ha='center')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = joined_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = joined_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute missing values in numerical columns with the column mean\n",
    "for col in numerical_cols:\n",
    "    joined_df[col].fillna(joined_df[col].mean(), inplace=True)\n",
    "\n",
    "# Impute missing values in categorical columns with the column mode\n",
    "for col in categorical_cols:\n",
    "    joined_df[col].fillna(joined_df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "joined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Batch size for batch processing\n",
    "batch_size = 2000\n",
    "\n",
    "# Function for advanced text preprocessing\n",
    "def advanced_preprocessing(doc):\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Initialize an empty list to hold the preprocessed texts\n",
    "preprocessed_texts = []\n",
    "\n",
    "# Perform batch processing\n",
    "for doc in nlp.pipe(joined_df['statement'].values, batch_size=batch_size):\n",
    "    preprocessed_texts.append(advanced_preprocessing(doc))\n",
    "\n",
    "# Add the preprocessed texts back to the DataFrame\n",
    "joined_df['statement_advanced'] = preprocessed_texts\n",
    "\n",
    "  from transformers import BertTokenizer, BertModel\n",
    "  import torch\n",
    "  import numpy as np\n",
    "\n",
    "  # Initialize the BERT tokenizer and model\n",
    "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "  model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "  # Function to get BERT embeddings for a batch of text\n",
    "  def get_bert_embeddings_for_batch(text_batch):\n",
    "      inputs = tokenizer(text_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "      outputs = model(**inputs)\n",
    "      embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "      return embeddings\n",
    "\n",
    "  # Batch size for BERT embeddings\n",
    "  batch_size = 500\n",
    "\n",
    "  # Initialize an empty list to hold the BERT embeddings\n",
    "  bert_embeddings = []\n",
    "\n",
    "  # Loop through the DataFrame in batches\n",
    "  for i in range(0, len(joined_df), batch_size):\n",
    "      text_batch = joined_df['statement_advanced'].iloc[i:i+batch_size].tolist()\n",
    "      embeddings_batch = get_bert_embeddings_for_batch(text_batch)\n",
    "      bert_embeddings.extend(embeddings_batch)\n",
    "\n",
    "  # Convert the list of embeddings to a NumPy array\n",
    "  bert_embeddings = np.array(bert_embeddings)\n",
    "\n",
    "  # Add the BERT embeddings back to the DataFrame\n",
    "  joined_df['bert_embeddings'] = list(bert_embeddings)\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Initialize TPU\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "# Create a distribution strategy\n",
    "tpu_strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "# Model architecture with dropout and regularization\n",
    "with tpu_strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(768,)),  # BERT embeddings size\n",
    "        layers.Reshape((1, 768)),\n",
    "        layers.Bidirectional(layers.LSTM(50, return_sequences=True)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Bidirectional(layers.LSTM(25)),\n",
    "        layers.Dense(30, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Extract features and labels\n",
    "X = np.stack(joined_df['bert_embeddings'].to_numpy())\n",
    "y = joined_df['BinaryNumTarget'].values\n",
    "\n",
    "# Compute class weights\n",
    "unique_classes = np.unique(y)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y)\n",
    "class_weights_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    lstm_units = trial.suggest_int('lstm_units', 20, 50)\n",
    "    dense_units = trial.suggest_int('dense_units', 10, 30)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.4, 0.7)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    l1_reg = trial.suggest_float('l1_reg', 1e-6, 1e-4, log=True)\n",
    "    l2_reg = trial.suggest_float('l2_reg', 1e-6, 1e-4, log=True)\n",
    "\n",
    "    # Initialize variables for k-fold cross-validation\n",
    "    k = 5\n",
    "    kf = StratifiedKFold(n_splits=k)\n",
    "    val_accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Early stopping\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        # Model architecture with hyperparameters\n",
    "        with tpu_strategy.scope():\n",
    "            model = tf.keras.Sequential([\n",
    "                layers.Input(shape=(768,)),\n",
    "                layers.Reshape((1, 768)),\n",
    "                layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Bidirectional(layers.LSTM(lstm_units//2, kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))),\n",
    "                layers.Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg)),\n",
    "                layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stop], class_weight=class_weights_dict)\n",
    "\n",
    "        val_accuracies.append(history.history['val_accuracy'][-1])\n",
    "\n",
    "    return np.mean(val_accuracies)\n",
    "\n",
    "# Initialize Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Extract features and labels for final evaluation\n",
    "X = np.stack(joined_df['bert_embeddings'].to_numpy())\n",
    "y = joined_df['BinaryNumTarget'].values\n",
    "\n",
    "# Extract best parameters from Optuna study\n",
    "best_params = study.best_params\n",
    "\n",
    "# Initialize variables for Stratified K-Fold\n",
    "k = 5\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "val_accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Model architecture with hyperparameters from Step 4\n",
    "    with tpu_strategy.scope():\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Input(shape=(768,)),  # BERT embeddings size\n",
    "            layers.Reshape((1, 768)),\n",
    "            layers.Bidirectional(layers.LSTM(best_params['lstm_units'], return_sequences=True, kernel_regularizer=regularizers.l1_l2(l1=best_params['l1_reg'], l2=best_params['l2_reg']))),\n",
    "            layers.Dropout(best_params['dropout_rate']),\n",
    "            layers.Bidirectional(layers.LSTM(best_params['lstm_units']//2, kernel_regularizer=regularizers.l1_l2(l1=best_params['l1_reg'], l2=best_params['l2_reg']))),\n",
    "            layers.Dense(best_params['dense_units'], activation='relu', kernel_regularizer=regularizers.l1_l2(l1=best_params['l1_reg'], l2=best_params['l2_reg'])),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        # Using the Adam optimizer with the suggested learning rate\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping callback with restore_best_weights\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=64, callbacks=[early_stop], validation_split=0.1)\n",
    "\n",
    "    # Model predictions\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    y_pred_binary = np.round(y_pred)\n",
    "\n",
    "    # Display classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "    # Store the validation accuracy for this fold\n",
    "    val_accuracies.append(history.history['val_accuracy'][-1])\n",
    "\n",
    "# Print the mean validation accuracy across all folds\n",
    "print(\"Mean Validation Accuracy: \", np.mean(val_accuracies))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model predictions\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "y_pred_binary = np.round(y_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='b', lw=1, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Split the data into training+validation and test sets, ensuring it's stratified.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Training final model using X_train_val and y_train_val\n",
    "with tpu_strategy.scope():\n",
    "    final_model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(768,)),  # BERT embeddings size\n",
    "        layers.Reshape((1, 768)),\n",
    "        layers.Bidirectional(layers.LSTM(\n",
    "            best_params['lstm_units'], return_sequences=True,\n",
    "            kernel_regularizer=regularizers.l1_l2(l1=best_params['l1_reg'], l2=best_params['l2_reg'])\n",
    "        )),\n",
    "        layers.Dropout(best_params['dropout_rate']),\n",
    "        layers.Bidirectional(layers.LSTM(\n",
    "            best_params['lstm_units'] // 2,\n",
    "            kernel_regularizer=regularizers.l1_l2(l1=best_params['l1_reg'], l2=best_params['l2_reg'])\n",
    "        )),\n",
    "        layers.Dense(\n",
    "            best_params['dense_units'], activation='relu',\n",
    "            kernel_regularizer=regularizers.l1_l2(l1=best_params['l1_reg'], l2=best_params['l2_reg'])\n",
    "        ),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "    final_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Use Early stopping and class weights during training\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the final model with a validation split and early stopping\n",
    "history = final_model.fit(\n",
    "    X_train_val, y_train_val, epochs=10, batch_size=64,\n",
    "    validation_split=0.1, callbacks=[early_stop], class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "# Step 3: Evaluate the model on the test set\n",
    "y_pred = final_model.predict(X_test).flatten()\n",
    "y_pred_binary = np.round(y_pred)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting Training History\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
